{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "9yO1HOZhUhvl",
        "itLIZ28ava6n",
        "nGKFtnJi3xfw",
        "Hjwtqdwy4UMq",
        "irF0vPHPIsxl",
        "tr8Uw1fobB2p"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TINF-DeepfakeEvaluierung/Evaluation-von-Methoden-zur-Erkennung-von-Deepfakes/blob/main/DSP_FWA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "j9iGKojqY7j1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id3h9bntizpP",
        "outputId": "bd0cbd77-9bbe-4764-fd2f-4d5aa4d18bbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DSP-FWA'...\n",
            "remote: Enumerating objects: 117, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 117 (delta 11), reused 2 (delta 0), pack-reused 82\u001b[K\n",
            "Receiving objects: 100% (117/117), 161.07 MiB | 17.21 MiB/s, done.\n",
            "Resolving deltas: 100% (26/26), done.\n",
            "Updating files: 100% (27/27), done.\n",
            "Collecting opencv-python==3.*\n",
            "  Downloading opencv_python-3.4.18.65-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (58.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python==3.*) (1.25.2)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "Successfully installed opencv-python-3.4.18.65\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/TINF-DeepfakeEvaluierung/DSP-FWA.git\n",
        "!pip install opencv-python==3.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/DSP-FWA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKHvwn1xV3M1",
        "outputId": "d6145343-791b-4823-bf4d-a8587094977e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/DSP-FWA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import cv2, os, dlib, json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import normalize\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "from py_utils.face_utils import lib\n",
        "from py_utils.vid_utils import proc_vid as pv\n",
        "from py_utils.DL.pytorch_utils.models.classifier import SPPNet"
      ],
      "metadata": {
        "id": "HVNzjeTUr00u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Cloud\n",
        "# Authenticate.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Install Cloud Storage FUSE.\n",
        "!echo \"deb https://packages.cloud.google.com/apt gcsfuse-`lsb_release -c -s` main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!apt -qq update && apt -qq install gcsfuse\n",
        "# Mount a Cloud Storage bucket or location, without the gs:// prefix.\n",
        "mount_path = \"deepfake_detection_datasets\"  # or a location like \"my-bucket/path/to/mount\"\n",
        "local_path = f\"/content/{mount_path}\"\n",
        "\n",
        "!mkdir -p {local_path}\n",
        "!gcsfuse --implicit-dirs {mount_path} {local_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMgyQoUxmEPi",
        "outputId": "2c1e05ab-0583-4d90-c248-8014010f221c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  2659  100  2659    0     0  13277      0 --:--:-- --:--:-- --:--:-- 13295\n",
            "Warning: apt-key is deprecated. Manage keyring files in trusted.gpg.d instead (see apt-key(8)).\n",
            "OK\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mhttps://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\u001b[0m\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 10.4 MB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_2.0.1_amd64.deb ...\n",
            "Unpacking gcsfuse (2.0.1) ...\n",
            "Setting up gcsfuse (2.0.1) ...\n",
            "{\"timestamp\":{\"seconds\":1715263839,\"nanos\":56114705},\"severity\":\"INFO\",\"message\":\"Start gcsfuse/2.0.1 (Go version go1.22.1) for app \\\"\\\" using mount point: /content/deepfake_detection_datasets\\n\"}\n",
            "{\"timestamp\":{\"seconds\":1715263839,\"nanos\":56509500},\"severity\":\"INFO\",\"message\":\"GCSFuse mount command flags: {\\\"AppName\\\":\\\"\\\",\\\"Foreground\\\":false,\\\"ConfigFile\\\":\\\"\\\",\\\"MountOptions\\\":{},\\\"DirMode\\\":493,\\\"FileMode\\\":420,\\\"Uid\\\":-1,\\\"Gid\\\":-1,\\\"ImplicitDirs\\\":true,\\\"OnlyDir\\\":\\\"\\\",\\\"RenameDirLimit\\\":0,\\\"CustomEndpoint\\\":null,\\\"BillingProject\\\":\\\"\\\",\\\"KeyFile\\\":\\\"\\\",\\\"TokenUrl\\\":\\\"\\\",\\\"ReuseTokenFromUrl\\\":true,\\\"EgressBandwidthLimitBytesPerSecond\\\":-1,\\\"OpRateLimitHz\\\":-1,\\\"SequentialReadSizeMb\\\":200,\\\"MaxRetrySleep\\\":30000000000,\\\"StatCacheCapacity\\\":20460,\\\"StatCacheTTL\\\":60000000000,\\\"TypeCacheTTL\\\":60000000000,\\\"HttpClientTimeout\\\":0,\\\"MaxRetryDuration\\\":-1000000000,\\\"RetryMultiplier\\\":2,\\\"LocalFileCache\\\":false,\\\"TempDir\\\":\\\"\\\",\\\"ClientProtocol\\\":\\\"http1\\\",\\\"MaxConnsPerHost\\\":100,\\\"MaxIdleConnsPerHost\\\":100,\\\"EnableNonexistentTypeCache\\\":false,\\\"StackdriverExportInterval\\\":0,\\\"OtelCollectorAddress\\\":\\\"\\\",\\\"LogFile\\\":\\\"\\\",\\\"LogFormat\\\":\\\"json\\\",\\\"ExperimentalEnableJsonRead\\\":false,\\\"DebugFuseErrors\\\":true,\\\"DebugFuse\\\":false,\\\"DebugFS\\\":false,\\\"DebugGCS\\\":false,\\\"DebugHTTP\\\":false,\\\"DebugInvariants\\\":false,\\\"DebugMutex\\\":false}\"}\n",
            "{\"timestamp\":{\"seconds\":1715263839,\"nanos\":56616349},\"severity\":\"INFO\",\"message\":\"GCSFuse mount config flags: {\\\"CreateEmptyFile\\\":false,\\\"Severity\\\":\\\"INFO\\\",\\\"Format\\\":\\\"json\\\",\\\"FilePath\\\":\\\"\\\",\\\"LogRotateConfig\\\":{\\\"MaxFileSizeMB\\\":512,\\\"BackupFileCount\\\":10,\\\"Compress\\\":true},\\\"MaxSizeMB\\\":-1,\\\"CacheFileForRangeRead\\\":false,\\\"CacheDir\\\":\\\"\\\",\\\"TtlInSeconds\\\":-9223372036854775808,\\\"TypeCacheMaxSizeMB\\\":4,\\\"StatCacheMaxSizeMB\\\":-9223372036854775808,\\\"EnableEmptyManagedFolders\\\":false,\\\"ConnPoolSize\\\":1}\"}\n",
            "{\"timestamp\":{\"seconds\":1715263840,\"nanos\":146561108},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ./ckpt\n",
        "!cp \"/content/deepfake_detection_datasets/DSP-FWA_(model)/SPP-res50.pth\" ./ckpt/"
      ],
      "metadata": {
        "id": "mrMc9zV5ucIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_num = 10\n",
        "# Employ dlib to extract face area and landmark points\n",
        "front_face_detector = dlib.get_frontal_face_detector()\n",
        "lmark_predictor = dlib.shape_predictor('./dlib_model/shape_predictor_68_face_landmarks.dat')\n",
        "\n",
        "def im_test(net, im, input_size):\n",
        "    face_info = lib.align(im[:, :, (2,1,0)], front_face_detector, lmark_predictor)\n",
        "    # Samples\n",
        "    if len(face_info) != 1:\n",
        "        prob = -1\n",
        "    else:\n",
        "        _, point = face_info[0]\n",
        "        rois = []\n",
        "        for i in range(sample_num):\n",
        "            roi, _ = lib.cut_head([im], point, i)\n",
        "            rois.append(cv2.resize(roi[0], (input_size, input_size)))\n",
        "\n",
        "        # vis_ = np.concatenate(rois, 1)\n",
        "        # cv2.imwrite('vis.jpg', vis_)\n",
        "\n",
        "        bgr_mean = np.array([103.939, 116.779, 123.68])\n",
        "        bgr_mean = bgr_mean[np.newaxis, :, np.newaxis, np.newaxis]\n",
        "        bgr_mean = torch.from_numpy(bgr_mean).float().cuda()\n",
        "\n",
        "        rois = torch.from_numpy(np.array(rois)).float().cuda()\n",
        "        rois = rois.permute((0, 3, 1, 2))\n",
        "        prob = net(rois - bgr_mean)\n",
        "        prob = F.softmax(prob, dim=1)\n",
        "        prob = prob.data.cpu().numpy()\n",
        "        prob = 1 - np.mean(np.sort(prob[:, 0])[np.round(sample_num / 2).astype(int):])\n",
        "    return prob, face_info\n",
        "\n",
        "def setup(arch, layers):\n",
        "    num_class = 2\n",
        "    if arch.lower() == 'sppnet':\n",
        "        net = SPPNet(backbone=layers, num_class=num_class)\n",
        "    net = net.cuda()\n",
        "    net.eval()\n",
        "    return net\n",
        "\n",
        "def predict_deepfake_video(input, arch=\"sppnet\", layers=50, input_size=224, save_dir=\"./ckpt/\", ckpt_name=\"SPP-res50.pth\"):\n",
        "    net = setup(arch, layers)\n",
        "    model_path = os.path.join(save_dir, ckpt_name)\n",
        "    if os.path.isfile(model_path):\n",
        "        #print(\"=> loading checkpoint '{}'\".format(model_path))\n",
        "        checkpoint = torch.load(model_path)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        net.load_state_dict(checkpoint['net'])\n",
        "        #print(\"=> loaded checkpoint '{}' (epoch {})\".format(model_path, start_epoch))\n",
        "    else:\n",
        "        raise ValueError(\"=> no checkpoint found at '{}'\".format(model_path))\n",
        "\n",
        "    f_path = input\n",
        "    #print('\\nTesting: ' + f_path)\n",
        "    suffix = f_path.split('.')[-1]\n",
        "    if suffix.lower() in ['jpg', 'png', 'jpeg', 'bmp', 'tif', 'nef', 'raf']:\n",
        "        im = cv2.imread(f_path)\n",
        "        if im is None:\n",
        "            prob = -1\n",
        "        else:\n",
        "            prob, face_info = im_test(net, im, input_size)\n",
        "        print(prob)\n",
        "\n",
        "    elif suffix.lower() in ['mp4', 'avi', 'mov']:\n",
        "        # Parse video\n",
        "        imgs, frame_num, fps, width, height = pv.parse_vid(f_path)\n",
        "        total_frames_to_process = min(300, len(imgs))\n",
        "        probs = []\n",
        "        for fid, im in enumerate(imgs):\n",
        "          if fid >= total_frames_to_process:\n",
        "            break\n",
        "\n",
        "          prob, face_info = im_test(net, im, input_size)\n",
        "          if prob != -1:\n",
        "            probs.append(1-prob)\n",
        "          else:\n",
        "            # if the model cant find a face, the result is 0.5 which can be interpreted as guessing Fake or Real\n",
        "            probs.append(0.5)\n",
        "\n",
        "        probs = np.mean(probs)\n",
        "        #print(f\"Probability: {probs}\")\n",
        "        return probs"
      ],
      "metadata": {
        "id": "EpMk8Mvss6-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FF++"
      ],
      "metadata": {
        "id": "9yO1HOZhUhvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffpp_dir = \"/content/deepfake_detection_datasets/FFPP/\"\n",
        "dataset = \"FaceForensics++\""
      ],
      "metadata": {
        "id": "7A3XS6r0UkO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images_per_class should be devidable by 6\n",
        "def detect_ffpp_deepfakes(ffpp_dir, images_per_class=30):\n",
        "    # Initialize arrays to store scores for real and fake videos.\n",
        "    scores_real = np.array([])\n",
        "    scores_fake = np.array([])\n",
        "    # Dictionaries to keep track of how many files have been processed in each subfolder.\n",
        "    manipulated_predicted_count = {}\n",
        "    original_predicted_count = {}\n",
        "\n",
        "    # Calculate the number of files to be processed per subfolder based on the folder type.\n",
        "    files_per_manipulated_subfolder = images_per_class / 6\n",
        "    files_per_original_subfolder = images_per_class / 2\n",
        "\n",
        "    # Total number of files targeted for processing from all subfolders.\n",
        "    total_files_to_process = 2 * images_per_class\n",
        "    processed_files = 0  # Counter for the number of files processed.\n",
        "\n",
        "    # Set up a progress bar with the total number of files to process.\n",
        "    pbar = tqdm(total=total_files_to_process)\n",
        "    pbar.set_description(f\"Processing: \")\n",
        "\n",
        "    # Walk through the directory structure of the given path.\n",
        "    for root, _, files in os.walk(ffpp_dir):\n",
        "        for file in files:\n",
        "            # Stop processing if the number of processed files meets the target.\n",
        "            if processed_files >= total_files_to_process:\n",
        "                break\n",
        "\n",
        "            # Process only MP4 video files.\n",
        "            if file.endswith('.mp4'):\n",
        "                vid_path = os.path.join(root, file)\n",
        "                rel_path = os.path.relpath(vid_path, ffpp_dir)\n",
        "                path_parts = rel_path.split(os.sep)\n",
        "\n",
        "                # Check if the video is from a manipulated or original sequence based on folder names.\n",
        "                is_manipulated = path_parts[0] == 'manipulated_sequences'\n",
        "                subfolder_path = os.path.join(path_parts[0], path_parts[1])\n",
        "\n",
        "                # Process videos from manipulated sequences.\n",
        "                if is_manipulated:\n",
        "                    # Ensure not to process more videos than the set limit for the subfolder.\n",
        "                    if manipulated_predicted_count.get(subfolder_path, 0) < files_per_manipulated_subfolder:\n",
        "                        faces_pred = predict_deepfake_video(vid_path)\n",
        "                        scores_fake = np.append(scores_fake, faces_pred)\n",
        "                        manipulated_predicted_count[subfolder_path] = manipulated_predicted_count.get(subfolder_path, 0) + 1\n",
        "                        processed_files += 1\n",
        "                        pbar.update(1)\n",
        "                # Process videos from original sequences.\n",
        "                else:\n",
        "                    if original_predicted_count.get(subfolder_path, 0) < files_per_original_subfolder:\n",
        "                        faces_pred = predict_deepfake_video(vid_path)\n",
        "                        scores_real = np.append(scores_real, faces_pred)\n",
        "                        original_predicted_count[subfolder_path] = original_predicted_count.get(subfolder_path, 0) + 1\n",
        "                        processed_files += 1\n",
        "                        pbar.update(1)\n",
        "\n",
        "        # Break the outer loop if the processing limit is reached.\n",
        "        if processed_files >= total_files_to_process:\n",
        "            break\n",
        "\n",
        "    # Close the progress bar upon completion.\n",
        "    pbar.close()\n",
        "    # Return the arrays containing the fake and real scores.\n",
        "    return scores_fake, scores_real"
      ],
      "metadata": {
        "id": "6ki14_uLI4ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_fake, score_real = detect_ffpp_deepfakes(ffpp_dir)"
      ],
      "metadata": {
        "id": "CXmOSjjNbFFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "becad286-af8a-490c-9596-2e685e0cccaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: :   0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "\n",
            "  0%|          | 0.00/97.8M [00:00<?, ?B/s]\u001b[A\n",
            " 13%|█▎        | 12.5M/97.8M [00:00<00:00, 131MB/s]\u001b[A\n",
            " 26%|██▌       | 25.3M/97.8M [00:00<00:00, 133MB/s]\u001b[A\n",
            " 39%|███▉      | 38.4M/97.8M [00:00<00:00, 135MB/s]\u001b[A\n",
            " 52%|█████▏    | 51.3M/97.8M [00:00<00:00, 121MB/s]\u001b[A\n",
            " 64%|██████▍   | 63.0M/97.8M [00:00<00:00, 114MB/s]\u001b[A\n",
            " 76%|███████▌  | 74.0M/97.8M [00:00<00:00, 111MB/s]\u001b[A\n",
            " 87%|████████▋ | 84.7M/97.8M [00:00<00:00, 109MB/s]\u001b[A\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 113MB/s]\n",
            "Processing: : 100%|██████████| 60/60 [1:16:41<00:00, 76.70s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Celeb-DF"
      ],
      "metadata": {
        "id": "itLIZ28ava6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "real_celeb_dir = \"/content/deepfake_detection_datasets/Celeb-DF/Celeb-real\"\n",
        "fake_celeb_dir = \"/content/deepfake_detection_datasets/Celeb-DF/Celeb-synthesis\"\n",
        "dataset = \"CelebDF\""
      ],
      "metadata": {
        "id": "OAcTEemYvfw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_celeb_deepfake(deepfake_folder, num_videos=5):\n",
        "  scores = np.array([]);\n",
        "\n",
        "  for deepfake in tqdm(islice(os.listdir(deepfake_folder), num_videos), total=num_videos):\n",
        "    vid_path = os.path.join(deepfake_folder, deepfake)\n",
        "    faces_pred = predict_deepfake_video(vid_path)\n",
        "\n",
        "    scores = np.append(scores, faces_pred)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "yGbsEyYnxoM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#score_real = detect_celeb_deepfake(real_celeb_dir)\n",
        "score_fake = detect_celeb_deepfake(fake_celeb_dir)"
      ],
      "metadata": {
        "id": "StREme-Kxu25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_fake"
      ],
      "metadata": {
        "id": "l93tGhtgst09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FaceAVCeleb"
      ],
      "metadata": {
        "id": "nGKFtnJi3xfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fakeavceleb_metadata = \"/content/deepfake_detection_datasets/FakeAVCeleb/meta_data.csv\"\n",
        "dataset = \"FakeAVCeleb\""
      ],
      "metadata": {
        "id": "KYoG0p7430Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files_per_class must be devidable by 10\n",
        "def detect_fakeAVCeleb_deepfake(metadata_file, videos_per_class=30):\n",
        "  metadata = pd.read_csv(metadata_file)\n",
        "\n",
        "  scores_real = np.array([]);\n",
        "  scores_fake = np.array([]);\n",
        "\n",
        "  # Filter for 'RealVideo-RealAudio' category\n",
        "  real_videos = metadata[metadata['type'] == 'RealVideo-RealAudio'].head(videos_per_class)\n",
        "\n",
        "  videos_per_fake_class = int(videos_per_class/10)\n",
        "  # Filter for 'FakeVideo-RealAudio' category and sample\n",
        "  fake_videos = metadata[metadata['type'] == 'FakeVideo-RealAudio']\n",
        "  sampled_fakes = fake_videos.groupby(['race', 'gender']).apply(\n",
        "        lambda x: x.sample(n=videos_per_fake_class, replace=False) if len(x) >= videos_per_fake_class else x).reset_index(drop=True)\n",
        "\n",
        "  # Concatenate real and sampled fake videos\n",
        "  final_metadata = pd.concat([real_videos, sampled_fakes]).reset_index(drop=True)\n",
        "\n",
        "  for index, properties in tqdm(final_metadata.iterrows(), total=len(final_metadata.index)):\n",
        "    vid_path = os.path.join(\"/content/deepfake_detection_datasets/\", properties['Unnamed: 9'], properties['path'])\n",
        "\n",
        "    faces_pred = predict_deepfake_video(vid_path)\n",
        "\n",
        "    if properties['method'] == \"real\":\n",
        "      scores_real = np.append(scores_real, faces_pred)\n",
        "    else:\n",
        "      scores_fake = np.append(scores_fake, faces_pred)\n",
        "\n",
        "  return scores_fake, scores_real"
      ],
      "metadata": {
        "id": "KCx1t4e336QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_fake, score_real = detect_fakeAVCeleb_deepfake(fakeavceleb_metadata)"
      ],
      "metadata": {
        "id": "Uaxod31y4Bwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2220791-00e7-49fc-d76d-2f7dd9b98c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 60/60 [09:50<00:00,  9.84s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DFDC"
      ],
      "metadata": {
        "id": "Hjwtqdwy4UMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfdc_dir = \"/content/deepfake_detection_datasets/DFDC/train_sample_videos\"\n",
        "dataset = \"DFDC\""
      ],
      "metadata": {
        "id": "jzEvxjbM4TeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# videos_per_class can not be bigger then 77 because there are only 77 real videos\n",
        "def detect_dfdc_deepfake(deepfake_folder, videos_per_class=30):\n",
        "  metadata_file = deepfake_folder + \"/metadata.json\"\n",
        "  video_count = {'REAL': 0, 'FAKE': 0}\n",
        "  pbar = tqdm(total=videos_per_class*2)\n",
        "\n",
        "  with open(metadata_file, 'r') as file:\n",
        "      metadata = json.load(file)\n",
        "\n",
        "  scores_real = np.array([]);\n",
        "  scores_fake = np.array([]);\n",
        "  for deepfake, properties in metadata.items():\n",
        "    if video_count[properties['label']] < videos_per_class:\n",
        "      vid_path = os.path.join(deepfake_folder, deepfake)\n",
        "      faces_pred = predict_deepfake_video(vid_path)\n",
        "\n",
        "      if properties['label'] == \"FAKE\":\n",
        "        scores_fake = np.append(scores_fake, faces_pred)\n",
        "        video_count['FAKE'] += 1\n",
        "      else:\n",
        "        scores_real = np.append(scores_real, faces_pred)\n",
        "        video_count['REAL'] += 1\n",
        "      pbar.update(1)\n",
        "\n",
        "  pbar.close()\n",
        "  return scores_fake, scores_real"
      ],
      "metadata": {
        "id": "XQRmrBOB4cDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_fake, score_real = detect_dfdc_deepfake(dfdc_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIUGmrGN4dio",
        "outputId": "b9626d5b-6c04-41cb-ee6c-1f06f47e102d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/60 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  2%|▏         | 1/60 [02:09<2:07:36, 129.77s/it]\u001b[A\n",
            "  3%|▎         | 2/60 [04:13<2:02:04, 126.28s/it]\u001b[A\n",
            "  5%|▌         | 3/60 [06:22<2:01:17, 127.68s/it]\u001b[A\n",
            "  7%|▋         | 4/60 [08:09<1:51:16, 119.22s/it]\u001b[A\n",
            "  8%|▊         | 5/60 [10:18<1:52:28, 122.70s/it]\u001b[A\n",
            " 10%|█         | 6/60 [12:26<1:52:17, 124.77s/it]\u001b[A\n",
            " 12%|█▏        | 7/60 [14:35<1:51:12, 125.89s/it]\u001b[A\n",
            " 13%|█▎        | 8/60 [16:44<1:50:04, 127.02s/it]\u001b[A\n",
            " 15%|█▌        | 9/60 [18:55<1:49:08, 128.40s/it]\u001b[A\n",
            " 17%|█▋        | 10/60 [21:04<1:47:06, 128.53s/it]\u001b[A\n",
            " 18%|█▊        | 11/60 [22:48<1:38:47, 120.97s/it]\u001b[A\n",
            " 20%|██        | 12/60 [24:59<1:39:17, 124.12s/it]\u001b[A\n",
            " 22%|██▏       | 13/60 [27:08<1:38:22, 125.59s/it]\u001b[A\n",
            " 23%|██▎       | 14/60 [29:18<1:37:15, 126.86s/it]\u001b[A\n",
            " 25%|██▌       | 15/60 [31:26<1:35:27, 127.27s/it]\u001b[A\n",
            " 27%|██▋       | 16/60 [33:36<1:33:52, 128.00s/it]\u001b[A\n",
            " 28%|██▊       | 17/60 [35:45<1:31:55, 128.27s/it]\u001b[A\n",
            " 30%|███       | 18/60 [37:57<1:30:32, 129.34s/it]\u001b[A\n",
            " 32%|███▏      | 19/60 [40:02<1:27:34, 128.15s/it]\u001b[A\n",
            " 33%|███▎      | 20/60 [42:07<1:24:51, 127.29s/it]\u001b[A\n",
            " 35%|███▌      | 21/60 [44:09<1:21:39, 125.62s/it]\u001b[A\n",
            " 37%|███▋      | 22/60 [46:18<1:20:05, 126.46s/it]\u001b[A\n",
            " 38%|███▊      | 23/60 [48:28<1:18:37, 127.51s/it]\u001b[A\n",
            " 40%|████      | 24/60 [50:36<1:16:43, 127.87s/it]\u001b[A\n",
            " 42%|████▏     | 25/60 [52:47<1:15:06, 128.76s/it]\u001b[A\n",
            " 43%|████▎     | 26/60 [54:56<1:12:54, 128.67s/it]\u001b[A\n",
            " 45%|████▌     | 27/60 [57:07<1:11:10, 129.41s/it]\u001b[A\n",
            " 47%|████▋     | 28/60 [59:15<1:08:53, 129.19s/it]\u001b[A\n",
            " 48%|████▊     | 29/60 [1:01:21<1:06:14, 128.22s/it]\u001b[A\n",
            " 50%|█████     | 30/60 [1:03:30<1:04:14, 128.47s/it]\u001b[A\n",
            " 52%|█████▏    | 31/60 [1:05:41<1:02:19, 128.96s/it]\u001b[A\n",
            " 53%|█████▎    | 32/60 [1:07:49<1:00:04, 128.72s/it]\u001b[A\n",
            " 55%|█████▌    | 33/60 [1:09:58<58:00, 128.91s/it]  \u001b[A\n",
            " 57%|█████▋    | 34/60 [1:12:06<55:45, 128.67s/it]\u001b[A\n",
            " 58%|█████▊    | 35/60 [1:14:17<53:52, 129.30s/it]\u001b[A\n",
            " 60%|██████    | 36/60 [1:16:22<51:10, 127.95s/it]\u001b[A\n",
            " 62%|██████▏   | 37/60 [1:18:31<49:13, 128.43s/it]\u001b[A\n",
            "  0%|          | 0/60 [1:26:28<?, ?it/s]\n",
            "\n",
            " 65%|██████▌   | 39/60 [1:22:49<45:06, 128.87s/it]\u001b[A\n",
            " 67%|██████▋   | 40/60 [1:24:34<40:33, 121.67s/it]\u001b[A\n",
            " 68%|██████▊   | 41/60 [1:26:45<39:23, 124.39s/it]\u001b[A\n",
            " 70%|███████   | 42/60 [1:28:47<37:07, 123.75s/it]\u001b[A\n",
            " 72%|███████▏  | 43/60 [1:30:57<35:36, 125.68s/it]\u001b[A\n",
            " 73%|███████▎  | 44/60 [1:33:06<33:46, 126.66s/it]\u001b[A\n",
            " 75%|███████▌  | 45/60 [1:35:12<31:36, 126.43s/it]\u001b[A\n",
            " 77%|███████▋  | 46/60 [1:37:21<29:40, 127.15s/it]\u001b[A\n",
            " 78%|███████▊  | 47/60 [1:39:31<27:45, 128.09s/it]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TIMIT"
      ],
      "metadata": {
        "id": "irF0vPHPIsxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timit_high_dir = \"/content/deepfake_detection_datasets/DeepfakeTIMIT/deepfakes_higher_quality\"\n",
        "timit_low_dir = \"/content/deepfake_detection_datasets/DeepfakeTIMIT/deepfakes_lower_quality\""
      ],
      "metadata": {
        "id": "d2C6LYCTIwcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_timit_deepfake(deepfake_folder, num_videos=60):\n",
        "  scores = np.array([]);\n",
        "\n",
        "  for deepfake in tqdm(islice(os.listdir(deepfake_folder), num_videos), total=num_videos):\n",
        "    vid_path = os.path.join(deepfake_folder, deepfake)\n",
        "    faces_pred = predict_deepfake_video(vid_path)\n",
        "\n",
        "    scores = np.append(scores, faces_pred)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "AfQkzIfkIuej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title High Quality\n",
        "dataset = \"TIMIT (Hohe Auflösung\"\n",
        "\n",
        "score_fake = detect_timit_deepfake(timit_high_dir)\n",
        "score_real = np.array([])"
      ],
      "metadata": {
        "id": "AYlY0FRUJBBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Low Quality\n",
        "dataset = \"TIMIT (Niedrige Auflösung\"\n",
        "\n",
        "score_fake = detect_timit_deepfake(timit_low_dir)\n",
        "score_real = np.array([])"
      ],
      "metadata": {
        "id": "6sh67oC1YwFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d032cbc3-cf4b-4bde-e66e-4cdeeec877a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/60 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 60/60 [09:38<00:00,  9.64s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VASA"
      ],
      "metadata": {
        "id": "tr8Uw1fobB2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vasa_dir = \"/content/deepfake_detection_datasets/VASA-1\"\n",
        "dataset = \"VASA-1\""
      ],
      "metadata": {
        "id": "rtCcxPr5bBO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_vasa_deepfake(deepfake_folder):\n",
        "  scores = np.array([]);\n",
        "\n",
        "  for deepfake in tqdm(os.listdir(deepfake_folder)):\n",
        "    vid_path = os.path.join(deepfake_folder, deepfake)\n",
        "    faces_pred = predict_deepfake_video(vid_path)\n",
        "\n",
        "    scores = np.append(scores, faces_pred)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "bTSb19embPr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_fake = detect_vasa_deepfake(vasa_dir)\n",
        "score_real = np.array([])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xju2Chw7bb8q",
        "outputId": "7ebbc0b4-cdbb-4457-b105-764de96e9877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/15 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 15/15 [07:22<00:00, 29.51s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Diagrams"
      ],
      "metadata": {
        "id": "ghg8YAKoLoeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the default PLT style, in case it was changed for the confusion matrix\n",
        "plt.style.use('default')\n",
        "\n",
        "_, _, bars = plt.hist(score_real, range=(0,1), color='steelblue', edgecolor='black')\n",
        "plt.title(f'{net_model} ({train_db}) + {dataset} (echt)')\n",
        "plt.xlabel(f'Punktzahl\\nMittelwert: {score_real.mean()}')\n",
        "plt.bar_label(bars)\n",
        "plt.xlim(0, 1)\n",
        "plt.ylabel('#Videos')\n",
        "plt.show()\n",
        "\n",
        "_, _, bars = plt.hist(score_fake, range=(0,1), color='steelblue', edgecolor='black')\n",
        "plt.title(f'{net_model} ({train_db}) + {dataset} (manipuliert)')\n",
        "plt.xlabel(f'Punktzahl\\nMittelwert: {score_fake.mean()}')\n",
        "plt.bar_label(bars)\n",
        "plt.xlim(0, 1)\n",
        "plt.ylabel('#Videos')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oks5dDuDZ9By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Data\n",
        "TP = np.sum(score_fake > 0.6) # True Positives  => Fake Videos als fake Klassifiziert\n",
        "FN = np.sum(score_fake < 0.6) # False Negatives => Fake Videos als echt Klassifiziert\n",
        "FP = np.sum(score_real > 0.4) # False Positives => Echt Videos als fake Klassifiziert\n",
        "TN = np.sum(score_real < 0.4) # True Negatives  => Echt Videos als echt Klassifiziert\n",
        "\n",
        "# Create the confusion matrix\n",
        "confusion_matrix = np.array([[TP, FN], [FP, TN]])\n",
        "# Create the nomalized confusion matrix\n",
        "normalized_confusion_matrix = normalize(confusion_matrix, norm='l1', axis=1)"
      ],
      "metadata": {
        "id": "kLZXbtPQ31po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for the classes\n",
        "labels = [\"Fake\", \"Echt\"]\n",
        "\n",
        "# Create a heatmap\n",
        "sns.set(color_codes=False)\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='.0f', cmap='Blues', cbar=False, xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "plt.figure(1, figsize=(5,5))\n",
        "plt.title(f'{net_model} ({train_db}) + {dataset}')\n",
        "plt.ylabel(\"Wirkliche Labels\")\n",
        "plt.xlabel('Vorhergesagte Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AE6qy1km23QL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels for the classes\n",
        "labels = [\"Fake\", \"Echt\"]\n",
        "\n",
        "# Create a heatmap\n",
        "sns.set(color_codes=False)\n",
        "sns.heatmap(normalized_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', cbar=False, xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "plt.figure(1, figsize=(5,5))\n",
        "plt.title(f'{net_model} ({train_db}) + {dataset}')\n",
        "plt.ylabel(\"Wirkliche Labels\")\n",
        "plt.xlabel('Vorhergesagte Labels')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hd6VZyTX2tyt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}